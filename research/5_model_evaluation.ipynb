{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4d18c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import tempfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d99c840",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(override=True)\n",
    "\n",
    "mlflow_tracking_uri = os.getenv('MLFLOW_TRACKING_URI')\n",
    "dagshub_username = os.getenv('DAGSHUB_USERNAME')\n",
    "dagshub_token = os.getenv('DVC_SECRET_ACCESS_KEY')\n",
    "\n",
    "if mlflow_tracking_uri is None:\n",
    "    raise ValueError(\"mlflow_tracking_uri is not set in your environment!\")\n",
    "\n",
    "if dagshub_token is None:\n",
    "    raise ValueError(\"DAGSHUB_TOKEN is not set in your environment!\")\n",
    "\n",
    "if dagshub_username is None:\n",
    "    raise ValueError(\"DAGSHUB USERNAME is not set in your environment!\")\n",
    "\n",
    "os.environ[\"MLFLOW_TRACKING_PASSWORD\"] = dagshub_token\n",
    "\n",
    "os.environ['MLFLOW_TRACKING_URI'] = mlflow_tracking_uri\n",
    "\n",
    "os.environ['MLFLOW_TRACKING_USERNAME'] = dagshub_username"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8ca251d",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "40b1ed25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\Courses\\\\Udemy\\\\Complete MLOps Bootcamp\\\\Projects\\\\7_End to End Projects\\\\end-to-end-data-science-project'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "51d7f34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ModelEvaluationConfig:\n",
    "    root_dir: Path\n",
    "    test_data_path: Path\n",
    "    model_path: Path\n",
    "    metric_file_name: Path\n",
    "    target_column: str\n",
    "    mlflow_tracking_uri: str\n",
    "    all_params: list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d805bf64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.project.constants import *\n",
    "from src.project.utils.common import read_yaml, create_directories, save_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "76bf9af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self,\n",
    "        config_filepath = CONFIG_FILE_PATH,\n",
    "        params_filepath = PARAMS_FILE_PATH,\n",
    "        schema_filepath = SCHEMA_FILE_PATH):\n",
    "\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "        self.schema = read_yaml(schema_filepath)\n",
    "\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "    def get_model_evaluation_config(self) -> ModelEvaluationConfig:\n",
    "        config=self.config.model_evaluation\n",
    "        all_params = []\n",
    "        all_params.append({'RandomForestBest': self.params['RandomForestBest']})\n",
    "        all_params.append({'XGBoostBest': self.params['XGBoostBest']})\n",
    "        print(all_params)\n",
    "        schema=self.schema.TARGET_COLUMN\n",
    "\n",
    "        create_directories([config.root_dir])\n",
    "\n",
    "        model_evaluation_config=ModelEvaluationConfig(\n",
    "            root_dir=config.root_dir,\n",
    "            test_data_path=config.test_data_path,\n",
    "            model_path = config.model_path,\n",
    "            metric_file_name = config.metric_file_name,\n",
    "            target_column = schema.name,\n",
    "            all_params = all_params,\n",
    "            mlflow_tracking_uri=os.environ['MLFLOW_TRACKING_URI'] # type: ignore\n",
    "        )\n",
    "        return model_evaluation_config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cdc07641",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    classification_report, confusion_matrix\n",
    ")\n",
    "from urllib.parse import urlparse\n",
    "import mlflow\n",
    "\n",
    "import numpy as np\n",
    "import joblib\n",
    "from mlflow.models import infer_signature\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ca26dfb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow.sklearn\n",
    "import mlflow.xgboost\n",
    "\n",
    "class ModelEvaluation:\n",
    "    def __init__(self, config: ModelEvaluationConfig):\n",
    "        self.config = config\n",
    "\n",
    "    \n",
    "    def eval_metrics(self, actual, pred):\n",
    "        \n",
    "        \"\"\"Calculate classification metrics for Forest Cover Type dataset\"\"\"\n",
    "        accuracy = accuracy_score(actual, pred)\n",
    "        precision = precision_score(actual, pred)\n",
    "        recall = recall_score(actual, pred)\n",
    "        \n",
    "        f1 = f1_score(actual, pred)\n",
    "              \n",
    "        return accuracy, precision, recall, f1\n",
    "\n",
    "    def log_into_mlflow(self, model_name):\n",
    "        \"\"\"Log all models into MLflow\"\"\"\n",
    "         \n",
    "        mlflow.set_registry_uri(self.config.mlflow_tracking_uri)\n",
    "        tracking_url_type_store = urlparse(mlflow.get_tracking_uri()).scheme\n",
    "    \n",
    "        # Find all model files in the model directory\n",
    "        model_files = []\n",
    "        model_dir = Path(self.config.model_path)\n",
    "    \n",
    "        # Look for different model files\n",
    "        all_params_dict = {k: v for d in self.config.all_params for k, v in d.items()}\n",
    "        model_params = all_params_dict[f'{model_name}Best'] \n",
    "        for file_path in model_dir.glob(\"*.joblib\"):\n",
    "            if model_name in file_path.name:\n",
    "                model_files.append((model_name, str(file_path)))\n",
    "                break  # Only need the first match\n",
    "        print(model_files, model_params)\n",
    "      \n",
    "        # print('I am here:', model_dir)\n",
    "      \n",
    "        #If no specific model files found, use the default path\n",
    "        if not model_files:\n",
    "            model_files = [(\"DefaultModel\", self.config.model_path)]\n",
    "    \n",
    "        all_results = {}\n",
    "    \n",
    "        # Loop through each model without nesting runs\n",
    "        for model_name, model_path in model_files:\n",
    "            print(f\"\\n>>> Evaluating {model_name} <<<\")\n",
    "        \n",
    "            # Start a separate run for each model (not nested)\n",
    "            with mlflow.start_run(run_name=f\"{model_name}_evaluation\"):\n",
    "                try:\n",
    "                    # Load test data and model\n",
    "                    test_data = pd.read_csv(self.config.test_data_path)\n",
    "                    model = joblib.load(model_path)\n",
    "\n",
    "                    test_x = test_data.drop([self.config.target_column], axis=1)\n",
    "                    test_y = test_data[self.config.target_column]\n",
    "\n",
    "                    # Make predictions\n",
    "                    predictions = model.predict(test_x)\n",
    "                \n",
    "                    signature = infer_signature(test_x, test_y)\n",
    "\n",
    "                    # Calculate metrics\n",
    "                    accuracy, precision, recall, f1 = self.eval_metrics(test_y, predictions)\n",
    "                \n",
    "                    # Store results\n",
    "                    metrics = {\n",
    "                        \"model_name\": model_name,\n",
    "                        \"accuracy\": accuracy,\n",
    "                        \"precision\": precision,\n",
    "                        \"recall\": recall,\n",
    "                        \"f1_score\": f1,\n",
    "                    }\n",
    "                \n",
    "                    all_results[model_name] = metrics\n",
    "                \n",
    "                    # Log parameters if available\n",
    "                    if model_params != '':\n",
    "                        mlflow.log_params(model_params)\n",
    "                \n",
    "                    # Log metrics to MLflow\n",
    "                    mlflow.log_metric(\"accuracy\", accuracy)\n",
    "                    mlflow.log_metric(\"precision\", precision)\n",
    "                    mlflow.log_metric(\"recall\", recall)\n",
    "                    mlflow.log_metric(\"f1_score\", f1)\n",
    "                \n",
    "                    # Log classification report as artifact\n",
    "                    class_report = classification_report(test_y, predictions, output_dict=True)\n",
    "                    class_report_path = f\"{model_name}_classification_report.json\"\n",
    "                    with open(class_report_path, 'w') as f:\n",
    "                        json.dump(class_report, f, indent=2)\n",
    "                    mlflow.log_artifact(class_report_path)\n",
    "                    os.remove(class_report_path)  # Clean up\n",
    "                    \n",
    "                    # Log confusion Matrix as artifact\n",
    "                    conf_matrix = confusion_matrix(test_y, predictions)\n",
    "                    conf_matrix_df = pd.DataFrame(conf_matrix)\n",
    "                    conf_matrix_path = f\"{model_name}_conf_matrix.csv\"\n",
    "                    conf_matrix_df.to_csv(conf_matrix_path, index=True, header=True)\n",
    "                    mlflow.log_artifact(conf_matrix_path, \"metrics\")\n",
    "                    os.remove(conf_matrix_path)\n",
    "                \n",
    "                    # Log model based on tracking store type\n",
    "                    if tracking_url_type_store != 'file':\n",
    "                        # For remote tracking (DagsHub), use artifact logging          \n",
    "                        with tempfile.TemporaryDirectory() as temp_dir:\n",
    "                            # Save model as pickle file\n",
    "                            temp_model_path = os.path.join(temp_dir, f\"{model_name}.pkl\")\n",
    "                            joblib.dump(model, temp_model_path)  # Actually save the model\n",
    "                            mlflow.log_artifact(temp_model_path, \"model\")\n",
    "                    \n",
    "                            # Save and log model signature\n",
    "                            signature_path = os.path.join(temp_dir, f\"{model_name}_signature.txt\")\n",
    "                            with open(signature_path, 'w') as f:\n",
    "                                f.write(f\"Inputs: {signature.inputs.to_dict()}\\n\")\n",
    "                                f.write(f\"Outputs: {signature.outputs}\\n\")\n",
    "                            mlflow.log_artifact(signature_path, \"model\")\n",
    "                    \n",
    "                            # Log model metadata\n",
    "                            mlflow.log_param(\"model_type\", type(model).__name__)\n",
    "                            mlflow.log_param(\"sklearn_version\", sklearn.__version__)\n",
    "                    else:\n",
    "                        if model_name == 'RandomForest':\n",
    "                            mlflow.sklearn.log_model(model, \"model\")\n",
    "                        else:\n",
    "                            mlflow.xgboost.log_model(model, 'model')\n",
    "                          \n",
    "                except Exception as e:\n",
    "                    print(f\"Error evaluating {model_name}: {str(e)}\")\n",
    "                    continue\n",
    "    \n",
    "        #Save all results locally\n",
    "        if all_results:\n",
    "            results_path = Path(self.config.metric_file_name)\n",
    "            save_json(path=results_path, data=all_results)\n",
    "        \n",
    "        return all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3b1245da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-09-07 14:28:47,567: INFO: common: yaml file: config\\config.yaml loaded successfully]\n",
      "[2025-09-07 14:28:47,583: INFO: common: yaml file: params.yaml loaded successfully]\n",
      "[2025-09-07 14:28:47,631: INFO: common: yaml file: schema.yaml loaded successfully]\n",
      "[2025-09-07 14:28:47,641: INFO: common: created directory at: artifacts]\n",
      "[{'RandomForestBest': ConfigBox({'max_depth': 30, 'min_samples_leaf': 2, 'min_samples_split': 5, 'n_estimators': 300})}, {'XGBoostBest': ConfigBox({'colsample_bytree': 1.0, 'learning_rate': 0.2, 'max_depth': 6, 'n_estimators': 200, 'subsample': 0.7})}]\n",
      "[2025-09-07 14:28:47,648: INFO: common: created directory at: artifacts/model_evaluation]\n",
      "[('RandomForest', 'artifacts\\\\model_train\\\\RandomForest.joblib')] {'max_depth': 30, 'min_samples_leaf': 2, 'min_samples_split': 5, 'n_estimators': 300}\n",
      "\n",
      ">>> Evaluating RandomForest <<<\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Courses\\Udemy\\Complete MLOps Bootcamp\\Projects\\7_End to End Projects\\end-to-end-data-science-project\\venv\\lib\\site-packages\\mlflow\\types\\utils.py:452: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error evaluating RandomForest: name 'sklearn' is not defined\n",
      "🏃 View run RandomForest_evaluation at: https://dagshub.com/magnifiques/end-to-end-data-science-project.mlflow/#/experiments/0/runs/4dd3f9a6686a4cf890b82dc957960f14\n",
      "🧪 View experiment at: https://dagshub.com/magnifiques/end-to-end-data-science-project.mlflow/#/experiments/0\n",
      "[2025-09-07 14:28:59,010: INFO: common: json file saved at: artifacts\\model_evaluation\\metrics.json]\n",
      "[('XGBoost', 'artifacts\\\\model_train\\\\XGBoost.joblib')] {'colsample_bytree': 1.0, 'learning_rate': 0.2, 'max_depth': 6, 'n_estimators': 200, 'subsample': 0.7}\n",
      "\n",
      ">>> Evaluating XGBoost <<<\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Courses\\Udemy\\Complete MLOps Bootcamp\\Projects\\7_End to End Projects\\end-to-end-data-science-project\\venv\\lib\\site-packages\\mlflow\\types\\utils.py:452: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error evaluating XGBoost: name 'sklearn' is not defined\n",
      "🏃 View run XGBoost_evaluation at: https://dagshub.com/magnifiques/end-to-end-data-science-project.mlflow/#/experiments/0/runs/92cbc176893b4685947840853f6323ee\n",
      "🧪 View experiment at: https://dagshub.com/magnifiques/end-to-end-data-science-project.mlflow/#/experiments/0\n",
      "[2025-09-07 14:29:21,260: INFO: common: json file saved at: artifacts\\model_evaluation\\metrics.json]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    model_evaluation_config = config.get_model_evaluation_config()\n",
    "    model_evaluation = ModelEvaluation(config=model_evaluation_config)\n",
    "    \n",
    "    model_names = ['RandomForest', 'XGBoost']\n",
    "    for model_name in model_names:\n",
    "        model_evaluation.log_into_mlflow(model_name)\n",
    "except Exception as e:\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75725727",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
